{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama library for LLM integration\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty variable for init\n",
    "LLMmodel = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing between LLM models\n",
    "choice = input(\"What LLM Model do you want to use?\\n\\nChoices:\\n1. Qwen [Q] - 500M parameters\\n2. Dolphin-Phi [D] - 3B parameters\\n3. TERMINAL [T] - 3B parameters\\n\\nNOTE: The more parameters, the slower the generation\\n\\n> \")\n",
    "if choice == 'Q':\n",
    "    LLMmodel = \"qwen:0.5b\"\n",
    "elif choice == 'D':\n",
    "    LLMmodel = \"dolphin-phi:latest\"\n",
    "elif choice == 'T':\n",
    "    LLMmodel = \"terminal:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I am a text-based AI and do not have ears to hear input. However, you can try asking a question or using specific words or phrases that you can use to communicate with someone. If you need help with something else, feel free to ask!As an AI language model, I do not have feelings in the same way that humans do. However, I am here to assist you with any questions or tasks you may have. So how can I help you today?"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    s = input(\"\\n> \")\n",
    "\n",
    "    # implement streaming to increase speed on this low-spec device\n",
    "    stream = ollama.chat(\n",
    "        model=LLMmodel, # a very lightweight model\n",
    "        messages=[{'role': 'user', 'content': str(s)}], # user says {s}\n",
    "        stream=True, # streaming for live printing of LLM\n",
    "    )\n",
    "\n",
    "    for chunk in stream:\n",
    "        # print in chunks as they're being generated\n",
    "        print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This code imports the ollama library which, contrary to popular belief, does **NOT** use an API. The NLP models run directly on the device, which is why I used some very lightweight and easy-to-run models like Qwen and Dolphin-Phi.\n",
    "\n",
    "# Requirements\n",
    "There needs to be some setup to get this script working. First, head over to https://ollama.com/download and install ollama onto your device. Then, open cmd prompt and type in `pip install ollama`.\n",
    "After installing ollama, type in the command prompt: `ollama run qwen:0.5b` and `ollama run dolphin-phi`\n",
    "\n",
    "# Some Issues\n",
    "In IPYNB, from what I can see, there is no word wrap to the LLM's responses, so it's a little inconvenient. This script is best ran from a console rather than just VSCode.\n",
    "\n",
    "It's also worth noting that it doesn't have a memory, but that's very easily achievable with a single json file and a few lines of code.\n",
    "\n",
    "# Things I wanted to do\n",
    "I'd expand on this script by adding a memory and increasing the number of models (Needs some better hardware)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: _NSR SDS_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
